{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code aus \"Neuronale Netze selbst programmieren,\n",
    "# ein verstÃ¤ndlicher Einstieg mit Python\"\n",
    "# von Tariq Rashid , O'Reilly\n",
    "# license GPLv2\n",
    "\n",
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "import os\n",
    "# helper to load data from PNG image files\n",
    "import imageio\n",
    "# glob helps select multiple files using patterns\n",
    "import glob\n",
    "# saving the NN \n",
    "import pickle\n",
    "#  !pip install dill\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "\n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each inpzt, hidden oputput layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "\n",
    "        # link weight matrices, wih and who \n",
    "        # weights inside the arrays aer w_i_j, where link is from node i to node j in the next layer\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # acitivation function is the sigmoid funciton\n",
    "        #self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        #self.inverse_activation_function = lambda x: scipy.special.logit(x)\n",
    "        \n",
    "        # performance \n",
    "        self.performance = 0\n",
    "        \n",
    "        # epochs\n",
    "        self.epochs = 0\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def activation_function(self, arr):\n",
    "        arrX = 0\n",
    "        for x in arr:\n",
    "            arrY = 0\n",
    "            for y in x:\n",
    "                arr[arrX][arrY] = scipy.special.expit(y)\n",
    "                arrY += 1\n",
    "                pass\n",
    "            arrX += 1\n",
    "            pass\n",
    "        return arr\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer        \n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)      \n",
    "        \n",
    "        # calculate signals final output layer        \n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "\n",
    "    \n",
    "    # backquery the neural network\n",
    "    # we'll use the same termnimology to each item, \n",
    "    # eg target are the values at the right of the network, albeit used as input\n",
    "    # eg hidden_output is the signal to the right of the middle nodes\n",
    "    def backquery(self, targets_list):\n",
    "        # transpose the targets list to a vertical array\n",
    "        final_outputs = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate the signal into the final output layer\n",
    "        final_inputs = self.inverse_activation_function(final_outputs)\n",
    "\n",
    "        # calculate the signal out of the hidden layer\n",
    "        hidden_outputs = numpy.dot(self.who.T, final_inputs)\n",
    "        # scale them back to 0.01 to .99\n",
    "        hidden_outputs -= numpy.min(hidden_outputs)\n",
    "        hidden_outputs /= numpy.max(hidden_outputs)\n",
    "        hidden_outputs *= 0.98\n",
    "        hidden_outputs += 0.01\n",
    "        \n",
    "        # calculate the signal into the hidden layer\n",
    "        hidden_inputs = self.inverse_activation_function(hidden_outputs)\n",
    "        \n",
    "        # calculate the signal out of the input layer\n",
    "        inputs = numpy.dot(self.wih.T, hidden_inputs)\n",
    "        # scale them back to 0.01 to .99\n",
    "        inputs -= numpy.min(inputs)\n",
    "        inputs /= numpy.max(inputs)\n",
    "        inputs *= 0.98\n",
    "        inputs += 0.01\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7165749/open-file-in-a-relative-location-in-python\n",
    "# relative path to files\n",
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "# For accessing the file in the parent folder of the current folder\n",
    "# small test data for quick calculations\n",
    "#training_data_path = os.path.join(fileDir, '../../trainingdata/testdata_mnist/mnist_train_100.csv')\n",
    "#test_data_path = os.path.join(fileDir, '../../trainingdata/testdata_mnist/mnist_test_10.csv')\n",
    "# real test data (60'000 training data / 10'000 test data)\n",
    "training_data_path = os.path.join(fileDir, '../../trainingdata/testdata_mnist/mnist_train.csv')\n",
    "test_data_path = os.path.join(fileDir, '../../trainingdata/testdata_mnist/mnist_test.csv') \n",
    "trained_NN_Path = os.path.join(fileDir, 'Trained_NN/Numbers/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist training csv file into a list\n",
    "training_data_file = open(training_data_path, 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist test csv file into a list\n",
    "test_data_file = open(test_data_path, 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes_list = [20]\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate_list = [0.6]\n",
    "\n",
    "# epochs - how often to train with the training data\n",
    "epochs_list = [1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network teststand\n",
    "class nnTestStand:\n",
    "\n",
    "    \n",
    "    # initialise the teststand\n",
    "    def __init__(self, neuralNetwork, epochs):\n",
    "        self.nn = neuralNetwork\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    # train the neural network\n",
    "    def trainNN(self):\n",
    "        self.nn.epochs = self.epochs\n",
    "\n",
    "        # epochs is the number of times the training data set is used for training\n",
    "        for e in range(self.epochs):\n",
    "\n",
    "            # go through all records in the training data set\n",
    "            for record in training_data_list:\n",
    "                # split the record by the ',' commas\n",
    "                all_values = record.split(',')\n",
    "                # scale input to range 0.01 to 1.00\n",
    "                inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "                # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "                targets = numpy.zeros(self.nn.onodes) + 0.01\n",
    "                # all_values[0] is the target label for this record\n",
    "                targets[int(all_values[0])] = 0.99\n",
    "                self.nn.train(inputs, targets)\n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # test the neural network\n",
    "    def testNN(self):\n",
    "        # scorecard for how well the network performs\n",
    "        scorecard = []\n",
    "\n",
    "        # go through all the records in the test data set\n",
    "        for record in test_data_list:\n",
    "            # split the record by the ',' commas\n",
    "            all_values = record.split(',')\n",
    "            # correct answer is first value\n",
    "            correct_label = int(all_values[0])\n",
    "            # scale input to range 0.01 to 1.00\n",
    "            inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "            # query the network\n",
    "            outputs = self.nn.query(inputs)\n",
    "            # the index of the highest value corresponds to the label\n",
    "            label = numpy.argmax(outputs)\n",
    "            # append correct or incorrect to list\n",
    "            if (label == correct_label):\n",
    "                # network's answer matches correct answer, add 1 to scorecard\n",
    "                scorecard.append(1)\n",
    "            else:\n",
    "                # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "                scorecard.append(0)\n",
    "                pass\n",
    "\n",
    "            pass\n",
    "\n",
    "        # calculate the performance score (correct answers/ all answers)\n",
    "        scorecard_array = numpy.asarray(scorecard)\n",
    "        performance = scorecard_array.sum() / scorecard_array.size\n",
    "\n",
    "        # save in the NN object\n",
    "        self.nn.performance = performance\n",
    "\n",
    "        pass\n",
    "    \n",
    "    # open object in pickle from: https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence\n",
    "    def saveBetterPerformingNN(self):\n",
    "        # create filename and path \n",
    "        filename = \"numberNN_%snodes_%slr_%sepochs.nn\"%(self.nn.hnodes, self.nn.lr, self.nn.epochs)\n",
    "        fullFilePath = trained_NN_Path + filename\n",
    "        print(filename)\n",
    "\n",
    "        # check if the file already exists - the nn was calculated the second time\n",
    "        if(os.path.isfile(fullFilePath)):\n",
    "            # if yes - open the object\n",
    "            with open(fullFilePath, 'rb') as input:\n",
    "                nnOld = pickle.load(input)\n",
    "\n",
    "            # check if we have a higer precision \n",
    "            # if yes - save the new\n",
    "            # if no - nothing happens we keep the old one\n",
    "            print(\"old performance: %s   new Performance: %s\"%(nnOld.performance, self.nn.performance))\n",
    "            if(self.nn.performance > nnOld.performance):\n",
    "                # if yes - save the new\n",
    "                self.saveNN(fullFilePath)\n",
    "                pass\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        else:\n",
    "            # save the new calculated nn\n",
    "            self.saveNN(fullFilePath)\n",
    "            pass\n",
    "\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # save object in pickle from: https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence\n",
    "    def saveNN(self, path):\n",
    "        # create the file\n",
    "        with open(path, 'wb') as output:\n",
    "            # save the object\n",
    "            pickle.dump(self.nn, output, pickle.HIGHEST_PROTOCOL)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberNN_20nodes_0.6lr_1epochs.nn\n",
      "file exists\n",
      "old performance: 0.8825   new Performance: 0.8692\n",
      "worse perf\n",
      "numberNN_20nodes_0.6lr_1epochs.nn\n",
      "file exists\n",
      "old performance: 0.8825   new Performance: 0.8705\n",
      "worse perf\n",
      "numberNN_20nodes_0.6lr_1epochs.nn\n",
      "file exists\n",
      "old performance: 0.8825   new Performance: 0.8658\n",
      "worse perf\n"
     ]
    }
   ],
   "source": [
    "# repeat more than one time to find better NNs\n",
    "reputations = 3\n",
    "for reputation in range(reputations):\n",
    "\n",
    "    # go through all combinations for training (hidden_nodes x learning_rate x epochs)\n",
    "    for hidden_nodes in hidden_nodes_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            for epochs in epochs_list:\n",
    "                # create instance of neural network\n",
    "                n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "                # create a teststand for the neural network\n",
    "                teststand = nnTestStand(n, epochs)\n",
    "                # train the network\n",
    "                teststand.trainNN()\n",
    "                # test the NN / its performance\n",
    "                teststand.testNN()\n",
    "                # save the NN if it perfomered better then before\n",
    "                teststand.saveBetterPerformingNN()      \n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.14008803 -2.34686792 -1.94717484  0.60195363 -2.99638615  0.96516582\n",
      "   4.95195537 -2.10409    -2.79330264 -0.99647159 -1.00205879 -1.89897535\n",
      "  -0.2018577  -0.64431372 -1.46790734 -2.85859329 -3.407411    0.94164958\n",
      "   0.36981203 -2.72029105]\n",
      " [-1.79090538 -0.47598779 -2.8014911  -2.56234764 -1.55141358  0.46474614\n",
      "   0.07867801  1.80623018 -1.87051617 -3.19304751 -6.17226062 -1.68664292\n",
      "  -0.92239425 -3.22416735  1.74674022  2.02442699 -1.77393822 -0.16747547\n",
      "  -1.0434277   2.5008998 ]\n",
      " [-4.35993393  0.07187475 -2.01819304 -0.33477557 -2.60210434 -2.52078707\n",
      "  -2.40477996  0.73904682 -0.04253336  2.49054605  0.88308428 -3.23884987\n",
      "  -0.13924058 -4.47470983  3.03103326  1.55311578 -3.39373875 -1.42773513\n",
      "  -0.41719651 -4.22944242]\n",
      " [ 2.83470792 -1.20451017 -1.91789458 -1.20138677 -2.97517504 -1.29116182\n",
      "  -1.87735444 -2.86049934 -1.87133042 -4.55852623 -0.50610457 -1.55085755\n",
      "  -1.29747837 -2.48963264 -4.35593514  3.31541059 -1.76025935 -2.93735326\n",
      "  -0.326404   -2.22558828]\n",
      " [-1.63783528  0.21585173 -1.72862583 -3.53459857  1.00055106 -0.44104585\n",
      "  -0.84159097 -4.63049562  3.52392447 -1.35576564 -1.65574294  3.23750062\n",
      "  -1.52238152 -3.76624325 -0.59265134 -3.379983   -3.15076997 -2.26889473\n",
      "  -2.41897722 -0.71111656]\n",
      " [ 2.34565387 -1.56055521  4.50034672  0.70205162 -3.28473739 -1.11325944\n",
      "  -5.58118604 -2.39421999 -0.41656401  0.12356216 -2.23991974 -0.73181679\n",
      "  -0.66426349 -0.75810509  0.13352711 -2.86933465 -2.24207825  4.21454992\n",
      "   1.5934326  -0.12946753]\n",
      " [-3.32370449 -1.91610128 -2.75694249 -0.65635666 -1.16532291 -0.80808953\n",
      "  -2.81528414  4.06944832 -3.19061738 -0.77673963 -1.88994606  2.97426208\n",
      "   0.45095049 -0.76895147 -1.65798638 -4.41623342 -2.42035215 -0.71808617\n",
      "  -3.07533751 -3.61594299]\n",
      " [-2.67021292 -1.85414771  1.73309873 -1.84973109  4.11949311  0.03001149\n",
      "  -0.02192793 -2.0780533  -2.79786585 -2.58786206 -2.32339201 -0.09663088\n",
      "  -2.94874186 -2.51607388  0.58751203  0.79412238  4.25461185 -0.36155564\n",
      "  -1.31614368 -2.39546493]\n",
      " [-2.30328107  2.79284275 -3.12562207  0.21218259 -3.38519311 -0.03770141\n",
      "  -3.29089277 -3.46212973 -4.7307423  -3.65892191  1.3802035  -2.60512337\n",
      "  -1.01265219 -1.02949698 -3.16277433 -2.78711488 -2.67018036 -2.88408369\n",
      "  -1.86878312  0.5275469 ]\n",
      " [ 0.29225856 -0.0470209  -3.04855688 -2.95560809 -3.63706583  0.60160093\n",
      "  -0.88933423 -2.83244919  4.28123472 -1.91924185 -0.18568776 -3.84235403\n",
      "  -0.36689246 -2.48282755 -2.3676912  -3.80789607  2.32478526 -2.74823124\n",
      "  -2.57550621 -0.04156373]]\n"
     ]
    }
   ],
   "source": [
    "print(n.who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
